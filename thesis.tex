% !TeX root = thesis.tex
% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
%\documentclass[]{report}
\documentclass[english, LaM, oneside]{sapthesis}%remove "english" for a thesis written in Italian

%\usepackage[utf8]{inputenx}
%\usepackage{xcolor}
%\usepackage{indentfirst}
%\usepackage{microtype}

%\usepackage{lettrine}
\linespread{0.9}

% This can be used to make space between section names more compact. The titlesec package allows changing how chapters are displayed, numerated, etc.
%\usepackage[compact]{titlesec}
% to get the bibliography in the toc
\usepackage[nottoc,notlot,notlof,chapter]{tocbibind}

%\newcommand{\thesistitle}{Reward shaping in RL for {\sc LTL}$_f$/{\sc LDL}$_f$ Goals: Theory and Practice}
\newcommand{\thesistitle}{Reinforcement Learning for {\sc LTL}$_f$/{\sc LDL}$_f$ Goals: Theory and Implementation}

\usepackage{index}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage[round]{natbib}
\bibliographystyle{plainnat}

\usepackage[intoc,refpage]{nomencl} %refeq
\makenomenclature

%\usepackage{qtree}
% The algorithm packages have to be after hyperref.
\usepackage{algorithm}
\usepackage{algpseudocode}


\usepackage{mathtools}
\usepackage{amssymb, amsmath, amsthm}
\usepackage{xspace, float, graphicx, pstricks}

\usepackage{caption}% http://ctan.org/pkg/caption
\captionsetup[ruled]{labelsep=period}
\makeatletter
\@addtoreset{algorithm}{chapter}% algorithm counter resets every chapter
\makeatother
\renewcommand{\thealgorithm}{\thechapter.\arabic{algorithm}}% Algorithm # is <chapter>.<algorithm>


\usepackage{subcaption}
%\usepackage[autostyle]{csquotes}  
\usepackage{graphicx}
%\graphicspath{{./images/}}
\usepackage{rotating}

%\usepackage{tikz}
%\usetikzlibrary{automata}
%\usetikzlibrary{positioning}

\usepackage{listings}
% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}

\usepackage{accsupp}    
\newcommand{\noncopynumber}[1]{
	\BeginAccSupp{method=escape,ActualText={}}
	#1
	\EndAccSupp{}
}
\lstdefinestyle{Python}{
	language        = Python,
	backgroundcolor=\color{backcolour},
	basicstyle      = \ttfamily,
	keywordstyle    = \color{deepblue},
	stringstyle     = \color{deepgreen},
	commentstyle    = \color{codegray}\ttfamily,
	numberstyle=\tiny\color{codegray}\noncopynumber,
	columns=flexible,
	numbers=left,
	stepnumber=1
}

\hypersetup{
	hyperfootnotes=true,            
	bookmarks=true,         
	colorlinks=true,
	linkcolor=red,
	linktoc=page,
	anchorcolor=black,
	citecolor=red,
	urlcolor=blue,
	pdftitle={\thesistitle},
	pdfsubject = {Master Thesis, University of Rome "La Sapienza"},
	pdfauthor={Marco Favortio},
	pdfkeywords={thesis, sapienza, roma, university, marco favorito}
	pdfauthor = {\textcopyright\ \today\ Marco Favorito},
}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter] % reset theorem numbering for each chapter
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{example}{Example}[chapter]

%opening
\title{\thesistitle}
\author{Marco Favorito}
\IDnumber{1609890}
\course[]{Master in Engineering in Computer Science}
\courseorganizer{Facolt\`a di Ingegneria dell'Informazione, Informatica e Statistica}
\submitdate{2017/2018}
\copyyear{2018}
\advisor{Prof. Giuseppe De Giacomo}
\coadvisor{Prof. Luca Iocchi}
\authoremail{favorito.1609890@studenti.uniroma1.it}
\examdate{$20^{\text{th}}$ July 2018}
\examiner{Prof. Riccardo Rosati} \examiner{Prof. Silvia Bonomi} \examiner{Prof. Giorgio Grisetti}  \examiner{Prof. Massimo Mecella}  \examiner{Prof. Daniele Cono D'Elia}

\allowdisplaybreaks

\begin{document}
	\input{macros}
	
	\frontmatter	
	\maketitle
	
	\begin{abstract}
		MDPs extended with \LLf non-Markovian rewards
		have recently attracted interest as a way to specify rewards	declaratively. In this thesis, we discuss how a reinforcement learning agent can learn policies fulfilling \LLf goals.
		In particular we focus on the case where we have two separate representations of the world: one for the agent, using the
		(predefined, possibly low-level) features available to it, and
		one for the goal, expressed in terms of high-level (human-understandable) fluents. We formally define the problem and
		show how it can be solved. Moreover, we provide experimental evidence that keeping the RL agent feature space separated from the goal's can work in practice, showing interesting cases where the agent can indeed learn a policy that fulfills the \LLf goal using only its features (augmented
		with additional memory).
	\end{abstract}
	
%	\dedication{to the Mother of my Mother.}
%	\dedication{Writing is good, thinking is better.\\ Cleverness is good, patience is better.}
	
%	\begin{acknowledgments}
%	\end{acknowledgments}
	
	\tableofcontents
	
	\mainmatter
	\input{chapters/introduction}
	\input{chapters/logic}
	\input{chapters/flloat}
	\input{chapters/reduced}
	\input{chapters/reward-shaping}
	\input{chapters/rltg}
	\input{chapters/experiments}
	\input{chapters/conclusions}
	
%	\appendix 
%	\input{chapters/appendix-flloat}
%	\input{chapters/appendix-rltg}

	
	\backmatter
	\phantomsection
	
	\bibliography{bib.bib}
	

\end{document}