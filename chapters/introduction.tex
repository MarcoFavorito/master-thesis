\chapter{Introduction}
This chapter presents the outline of this thesis and summarizes motivations, goals, and achievements. In \Section{sect:intro-rl} we generally describe the field of Reinforcement Learning, by explaining what it is, the Markov Decision Process models (MDP) and the main challenges that arise in solving them. In \Section{sect:intro-rewarding-behaviors} we focus on a specific subset of Reinforcement Learning problem, known in literature as Non-Markovian Reward Decision Process (NMRDP), that defines the background for our work. We explicitly declare our goals and the reached achievements in \Section{sect:intro-goals} and \Section{sect:intro-achievements}, respectively. Finally, the chapter ends with \Section{sect:intro-structure-thesis} by summarizing the structure of the thesis.

\section{Reinforcement Learning}\label{sect:intro-rl}
Reinforcement learning is an area of Machine Learning where the learning comes from rewards and punishments \citep{Sutton:1998:IRL:551283}. It is concerned in how the learning entity, the \emph{agent}, interacting in an \emph{environment}, should take \emph{actions} so to maximize the observed \emph{reward}. The reward signal is observed after each action taken by the agent. The agent chooses actions depending on the current state of the environment. A solution to the reinforcement learning problem is a \emph{policy} which determines which action should
be executed in a given state in order to maximize the long term reward. An algorithm that tackles this kind of problem is called \emph{reinforcement learning algorithm}.

\medskip
The problem, due to its generality, is studied in many other disciplines, such as \emph{game theory}, \emph{control theory}, \emph{operations research}, \emph{multi-agent system}, and many others. Usually, in order to simplify the tractability of the problem, it is assumed that the environment can be modeled as a \emph{Markov Decision Process} (MDP). An environment behaves like an MDP if the Markov property is satified, which means that the state space representation in the algorithm captures enough details so that the optimal decisions can be made when the information about only the current state is available. 

\medskip
Even if the laws that determines the evolution of the systems and the rewards are unknown a priori, it is still possible to solve an MDP by making several simulations and gathering experiences about the visited states, the actions taken and the observed rewards. However, many challenges arise in this settings:
\begin{itemize}
	\item \emph{Exponential state space explosion}: due to the feature selection used for the state space encoding, every added feature yields exponential increase in the number of states, hence reducing the tractability of the problem.
	\item \emph{Exploration-Exploitation trade-off}: due to the former issue, reinforcement learning algorithm should be designed to avoid exploring irrelevant states in terms of expected reward, while preferring the ones with high expected reward (\emph{exploitation}). Furthermore, the algorithms should be sensitive to the local optima issue, a well-known in statistical learning literature (\emph{exploration}).
	\item \emph{Temporal credit assignment}: the agent should be able to foresee the effect of his actions (in terms of expected reward), due to the fact that, in many domains, the current reward is influenced by past decisions.
\end{itemize}

This problems lead to the use of heuristics and approximate solutions. A simple way to do reinforcement learning is to use exploration which is based on the current policy with a certain degree of randomness which deviates from such a policy.

\section{Rewarding behaviors}\label{sect:intro-rewarding-behaviors}
In some domains it could be of interest the study of rewards not depending on a single decision (like in MDPs) but depending on a \emph{sequence} of visited states and actions. For instance, we can reward an agent not only by reaching a goal state, but if the goal is reached while satisfying other properties of interest during the simulation or, in other words, if the agent satisfies some target behavior. It is clear that the the definition of MDP does not fit this problem, since the optimal decision in the current state depends from the \emph{history of states} that leads to the current state.

\medskip

This idea of rewarding behaviors has been proposed in \citep{bacchus1996rewarding}, by defining the \emph{Non-Markovian Reward Decision Process} (NMRDP), a variant of an MDP where the reward does not depend only from one transition of the environment but from a sequence of transitions. In order to specify the desired behaviors that the agent should learn, they defined a temporal logic formalisms called \PLTL (Past \LTL), which is able to speak about a sequence of property configurations over time, that we call \emph{traces}. The classic reinforcement learning algorithms does not work on an NMRDP; however, they propose a transformation from NMRDP into an \emph{expanded} MDP such that the solution of the MDP is also a solution of the original problem. Hence, in order to solve the NMRDP, we can run off-the-shelf RL algorithms over the transformed MDP; the learnt Markovian policy can be easily converted into an optimal policy for the NMRDP.

The trick here is that the transformed MDP is defined over an \emph{expanded state space}, which still contains the original state space but enriches it by labeling every state. The idea is that the labels should keep track in some way the (partial) satisfaction of the temporal formulas. As a result, every state in the transformed state space is replicated multiple times, marking the difference between different (relevant) histories terminating in that state.

\medskip
A similar transformation has been done in the following works: 
\citep{ThiebauxGSPK06} and \citep{gretton2014more}, where the temporal logic formalism was respectively \FLTL (a finite \LTL with future formulas) and \FstarLTL (a variant of \FLTL); 
in \citep{icarte2018teaching}, where they used \emph{Co-Safe} \LTL formulas \citep{Kupferman:2001:MCS:569028.569032, Lacerda:2015:OPG:2832415.2832470};
in \citep{CamachoCSM17, CamachoCSM17b}, where they used \LTLf (Linear Temporal Logic over finite traces) \citep{de2013linear}.
In \citep{AAAI1817342} the specification of temporal goals is done by \LTLf or \LDLf (Linear Dynamic Logic over finite traces) formulas.

The construction shown in \citep{AAAI1817342} is the foundation for this work.


\section{Goals}\label{sect:intro-goals}

The idea to define non-Markovian rewards is interesting and, as we've seen in the previous section, has becoming popular. However, the cited works are mainly focused on \emph{planning over NMRDP}, where the model is known (i.e. the transition function and the reward function). 

Instead, in this work we focused on reinforcement learning over NRMDP, specified by \LLf formulas; that is, we do not assume nothing about the transition function and the reward function, but we aim to let the agent to learn how to accomplish the temporal goals. The agent, differently from the planning domain, during the simulations, is in charge of doing actions, observing the new states and collecting rewards, without prior knowledge. It has to come up with a policy learnt from experience.

Our first goal is to give theoretical foundations about reinforcement learning for non-Markovian rewards expressed by \LLf formulas. \LDLf formalism is expressive as Monadic Second-Order logic (\MSO), allowing to express many kind of complex temporally extended goal.

Then, we explore the possibility to have two different separated representations of the world: one used by the agent where it learns; the other to talk about temporally extended goals in \LLf. This construction has several advantages, such as modularity of the feature design and a reduced state space used by the learning agent.

Due to the \emph{sparsity of rewards} (i.e. the reward is given to the agent only when the goal is reached), we should devise an way to help exploration of the state space, by ignoring non-relevant portion of the state space. This is a crucial step for speed-up the convergence rate of the learning process.

An important goal of this work is to look for an implementation of the theoretical settings explored, as well as an experimental evidence, to prove that our approach actually makes sense. In particular, we need a way to manage \LLf formalisms and the reinforcement learning for \LLf goals to let the agent to learn temporal goals.


\section{Achievements}\label{sect:intro-achievements}
The first contribution of this thesis is to devise a new algorithm for the translation from \LLf formulas to \DFA, without the need to first translate into an \NFA and then determinize it. We call it \LDLfToDFA. Actually, it is a variant of the \LDLfToNFA algorithm described in \citep{AAAI1817342}, but with the advantage to generate only the reachable states of the \DFA, which in practice yields better performances.

Then, we created the  Python package FLLOAT (From \LTLf and \LDLf tO auTomata) that deals with transformation from \LLf formulas to equivalent automata. This is a crucial step for the computation of the extended MDP from an NMRDP with \LLf rewards \citep{AAAI1817342}.

We formalized the problem of \emph{RL over NMRDP with \LLf}, by leveraging the result in \citep{AAAI1817342}. We reduce the problem to classical RL over an equivalent MDP, hence enabling the use of off-the-shelf RL algorithms to find a solution to the original problem.

Then, we focused on a two-fold representation of the world. As stated before, one is the agent's representation that the agent uses to learn. The other one is used for specifying temporally extended goals expressed in \LLf. We give motivations about this approach, and analyzed the implications and the main advantages. Hence, we defined a new problem, \emph{RL for \LLf goals}, and we provided a solution that reduces the problem to reinforcement learning over an MDP (analogously to what we did for \emph{RL over NMRDP}).

We devised a way to apply \emph{reward shaping} to this setting, by leveraging the particular structure of our solution. Reward shaping is a well-known technique in reinforcement learning to speed up the convergence rate and to help the agent to explore the state space more efficiently. In our setting, we shaping-reward the transitions that makes a step toward the satisfaction of the \LLf goals $\varphi$; this reduces to reward transitions where the progression of the associated automata $\automaton_{\varphi}$ become nearer to an accepting state. We design two techniques. One that requires the $\automaton_{\varphi}$ to be built, i.e. is known before the learning process, that we call \emph{off-line} reward shaping. The other, instead, builds the automaton from scratch, by observing the transitions made during the learning process, that we call \emph{on-the-fly} reward shaping.

As a practical contribution, we implemented RLTG (Reinforcement Learning for Temporal Goal), a Python framework for easily set up a reinforcement learning experiment with \LLf goals, by leveraging the above mentioned FLLOAT for the construction of the automata. It works both as a classic reinforcement learning framework and a \LLf goal-based framework. 

Finally, we designed experiments with some simulated RL environments and implemented them with FLLOAT and RLTG. We introduced \LLf goals to classic RL environments (e.g. \Breakout), and show that the agent learn a policy that accomplishes those goals.
In other words, they provided experimental evidence of the goodness and practicability of our construction.


\section{Structure of the Thesis}\label{sect:intro-structure-thesis}
The rest of the thesis is structured as follows:
\begin{itemize}
	\item In Chapter \ref{ch:logic} we describe the notions about temporal logic formalisms, that will be used for temporal goal specifications. We start from \LTL, \REGEX and then we move towards \LTLf and \LDLf, upon which our method is built on. Moreover, we describe a new algorithm for the conversion of \LLf formulas to equivalent automata;
	\item In Chapter \ref{ch:flloat} we describe FLLOAT, a software project that implements the translation from \LLf formulas to equivalent automata. Such translation is an important piece of our approach and for the implementations of RL experiments;
	\item In Chapter \ref{ch:rl} we introduce foundational concepts in reinforcement learning, MDPs and algorithm to find a solution. Then we move to NMRDPs and we explain describe how to do RL for NMRDP with \LLf goals;
	\item In Chapter \ref{ch:rl-llf-goals} the two-fold representation of the world, the agent's low-level one where the agent learns and the high-level one used for express \LLf formulas, is introduced. We discuss implications and the advantages of our approach, and formally define a new problem. We also describe a solution to the problem.
	\item In Chapter \ref{ch:reward-shaping} we apply reward shaping techniques to the setting explained in Chapter \ref{ch:rl}. We propose an automata-based reward shaping, in the sense that we based the positive/negative extra rewards by the transitions over the automata, which track the satisfaction of the \LLf goals;
	\item In Chapter \ref{ch:rltg} we present a reinforcement learning framework allowing easy implementation of the construction described in Chapter \ref{ch:rl} and \ref{ch:reward-shaping}.
	\item Chapter \ref{ch:experiments} describes the conducted experiments,5 giving evidence that our approach actually works.
	\item The thesis is concluded in Chapter \ref{ch:conclusions}. This chapter summarizes also the achievements of
	the thesis and discusses future works.
\end{itemize}

\section{Conclusion}
Part of this work has been submitted to an important paper conference and it is under review.