\chapter{Conclusion and Future Work}\label{ch:conclusions}
This chapter summarizes the thesis. It starts with a brief overview of the problem studied. Then we summarize the main contributions of the thesis. A section is dedicated to future directions of research. The document ends with final remarks.
\section{Overview}
This work addressed the problem of Reinforcement Learning in the context of NMRDP, i.e. when the rewards are non-Markovian and so they depends from a sequence of transitions. In the literature, the problem has been solved exclusively in the planning domain, by two steps. First, by expressing the temporal goals in a temporal logic formalism (e.g. \PLTL, \FLTL, \LTLf) and so by implicitly denoting the trajectories to be rewarded. Then, by devising an expansion of the original state space such that each primitive state is copied multiple times in order to capture the relevant histories for the satisfaction of the temporal goals, and making the transition model to satisfy the Markov property. This transformation into an expanded MDP allows to run classic reinforcement learning algorithms and so to implicitly learn the non-Markovian policy to solve the original problem.

Here, instead, we focused on doing Reinforcement Learning for NMRDPs.
We used the definition of the goals in NMRDP by using \LDLf formalism, that is expressive as \MSO logic. The transformation to an equivalent MDP is provided by results in \citep{AAAI1817342}. Moreover, we consider the case when can be useful to devise two different representations of the world. We considered a classical reinforcement learning problem, i.e. an MDP over a set of features of the world, that we called \emph{low-level} features
\footnote{We say \emph{low-level} features in the sense that these features are only responsible for the basic interaction of the agent with the environment, but still allowing to maximize the reward, or reach the goal in the MDP. Notice that it is only a way to refer about them, and no restriction is made over the MDP state space.}.
We are interested in defining non-Markovian rewards over this MDP about some high-level properties of the environment, that we called the \emph{fluents}, determined by some set of \emph{high-level} features of the world. This approach gives us several advantages, such as modularity of the system, learning over a reduced state space (minimal in terms of what is needed for the agent) and declarativeness of the target behaviors in terms of the fluents.

Moreover, we are interested in an implementation of these topics, by providing software that deals with the specification of \LLf formulas, the computation of the extended state space and the running of reinforcement learning algorithms that solve this problems.

\section{Main Contributions}
In this section we list the main contribution of the thesis.
\begin{itemize}
	\item Explanation of the transformation from \LLf formulas to automata, supported by several examples in every variant. Design of a new algorithm for the translation from \LLf formulas to \DFA (\LDLfToDFA, Algorithm \ref{alg:ldlf2dfa}), for implementation purposes. Actually, it is a variant of the \LDLfToNFA algorithm described in \citep{AAAI1817342}.
	
	\item Implementation of the Python package FLLOAT (From \LTLf and \LDLf tO auTomata). We described the main features and several use cases in Chapter \ref{ch:flloat}. It supports the representation of \PL and \LLf formulas, as well as their truth evaluation. As a core feature, it implements the transformation from \LLf formulas to equivalent automata, in the variant of \LDLfToNFA, \LDLfToDFA and on-the-fly automaton.
	
	\item As a theoretical contribution, we defined the problem of \emph{Reinforcement Learning over NMRDP with \LLf rewards}, by leveraging the results in \citep{AAAI1817342}. Indeed, the proposed solution reduces the problem to classic reinforcement learning over an equivalent MDP with an expanded state space. Since a optimal policy for the extended MDP can be transformed to an optimal policy for NMRDP, the problem reduces to find an optimal policy for the MDP.
	
	\item A relevant part of this work dealt with a two-fold representation of the world. One is the agent's representation that the agent uses to learn. The other one is used for specifying temporally extended goals expressed in \LLf. We formally defined the problem, that we call \emph{RL for \LLf goals}, and provide a solution. The idea is to reduce the new problem to an instance of \emph{RL over NMRDP with \LLf rewards}, which in turn can be reduced to plain RL over MDP. We formally describe our approach step by step.
	
	\item We devised a way to apply \emph{reward shaping} to this setting, by leveraging the particular structure of our solution. Indeed, the core observation is the following: the transitions that make a step toward the satisfaction of the \LLf goals can be seen as transitions that make a step toward an accepting state of $\automaton_\varphi$. Hence, by positively rewarding this transitions (and negatively the opposite ones), we help the agent to explore the extended state space, preventing the main issues caused by \emph{sparse rewards}.
	
	We design the reward shaping by considering the definition of \emph{potential based reward function} \citep{Ng:1999:PIU:645528.657613} and its property of policy invariance, i.e. the optimal policy does not change when applied this kind of reward shaping. The potential function associates each state of the automaton to a real number. The nearer the state to an accepting state, the higher the potential function evaluated in that state. We formally devised a procedure that, given $\automaton_\varphi$, returns a potential function defined over the states of $\automaton_\varphi$ (Algorithm \ref{alg:static-reward-shaping}) We call it \emph{static reward shaping} or \emph{off-line reward shaping}.
	
	We proposed a variant of off-line reward shaping that does not require $\automaton_\varphi$. In this approach, $\automaton_\varphi$ is built from scratch, during the learning process, by observing the transitions and collecting informations about the states of the automata and the value of the fluents that allow the transitionWe call this approach \emph{on-the-fly} reward shaping.
	
	\item We implemented RLTG (Reinforcement Learning for Temporal Goal), a Python framework for easily set up a reinforcement learning experiment with \LLf goals, by leveraging the above mentioned FLLOAT for the construction of the automata. It works both as a classic reinforcement learning framework and a \LLf goal-based framework, supporting the settings presented in Chapter \ref{ch:rl} and Chapter \ref{ch:rl-llf-goals}, as well as the reward shaping techniques explained in Chapter \ref{ch:reward-shaping}.
	
	We achieved an effective modularization of a reinforcement learning system, that allows the specification of temporal goals over a set of fluents, whose truth value is extracted from the RL environment by user-defined functions. We aimed to highly customizability of the components: features extraction of the features set of the agent and of the \LLf goals, mapping from features to fluents, specification of \LLf formulas to be satisfied by the agent. 
	
	The available RL algorithms are Q$(\lambda)$ and Sarsa$(\lambda)$, but the framework allow to easily implement new algorithms and seamlessly integrate them with the other components of the framework. It allow the user to pause the learning process in order to resume it later, to render the agent while simulating in the environment and to collect statistics about the learning.
	
	\item Finally, we designed experiments with simulated RL environments and implemented them with FLLOAT and RLTG. For each experiment, we specified a temporal goal with a \LLf formula.
	
	We started from a classic RL environment, \Breakout, and introduced a temporally extended goal (break columns/rows in a given order). We made the following observations: we noticed that the agent learnt the optimal policies in many different version of the temporal goal (e.g. break columns from left-to-right/right-to-left and break rows from top-to-bottom/bottom-to-top), even when considered both of them (but not mutually exclusive ones). Moreover, we compared the use of off-line reward shaping, on-the-fly reward shaping and any use of reward shaping, and we observed that the use of rerward shaping outperformed the configuration without reward shaping.
	
	Then we moved to the \Sapientino environment, and we specified two versions of the goal: one that the agent had to visit the colors of the grid in a given order \emph{relaxed}, and another, more restricting, that requires also that no bip action can be executed if the agent is not over a colored cell \emph{full}. For the relaxed temporal goal, the agent always learnt the optimal policy in a limited amount of time. In the full temporal goal, only in the configurations with reward shaping the agent actually accomplished the goal, which had not happened in the configuration without reward shaping. These experiments show that the reward shaping is necessary for complex temporal goal to be learnt in a reasonable amount of time, and that a slight different temporal specification might highly affect the exploration phase.
	
	The last experimented environment is \Minecraft, where the agent had to accomplish many tasks, each of them composed by many subtasks that might be shared among other tasks. We run the experiment with three tasks, and compared the different use of reward shaping, as we did in the \Breakout experiment. The result is that, still, the reward shaping configurations outperform the no-reward shaping configurations, in terms of time needed to learn every temporal goal. 
	Moreover, we noticed that the particular structure of the temporal goals might have introduced difficulties in the learning, due to the interleaving of different tasks. However, the agent has been able to learn a proper policy in a reasonable amount of time.
	
	

\end{itemize}

\section{Future Works}
There are many future directions that can be taken, due to the novelty of the work. Some of them are:
\begin{itemize}
	\item In this work we prevalently made a theoretical analysis of the problem and we have shown only some application at software level. The proposed framework is more general, and should be validated in many other domains, both simulated and physical ones (e.g. robotics), where there is the need to represent additional high-level knowledge to express complex goals;
	\item We did not focus our attention over the algorithmic side, but only on how the problem can be properly redefined, and relying on off-the-shelf reinforcement learning algorithms. It might be the case that the design of ad-hoc algorithms for this framework yields better performances. For instance, one could think about a better "explicit" exploration policy of the state space, in spite of "implicit" guidance by using reward shaping, and even some adaptation of state-of-the-art techniques, e.g. hierarchical reinforcement learning, curriculum learning, knowledge transfer and so on.
	\item Finally, it could be of interest to extend this work to the general framework of multi-agent systems and work on the challenges and issues that such extension might lead to.
\end{itemize}

\section{Final Remarks}
The topic of this work is of crucial importance: the need to face the dichotomy between the control of low-level features and the ability to reason about higher-level properties of the world is the core issue in many of the fields and applications in artificial intelligence, especially in robotics. This thesis aimed to address this issue in a particular case, by a theoretical analysis, but providing actual implementations as support for the proposed approach.